#+begin_html
---
layout: post
title: "Making an Instrument"
date: 2012-09-28 14:53
comments: true
categories: 
---
#+end_html

This covers the basics of creating an /instrument/ in Extempore. While
there are [[file:~/Documents/biott/org/_posts/2012-06-07-dsp-basics-in-extempore.org][other]] [[file:~/Documents/biott/org/_posts/2012-06-07-more-dsp-and-extempore-types.org][posts]] which cover audio DSP at a lower level---from
the basic building blocks of oscillators and filters, this tutorial
covers the process of building an instrument which can be played using
the conventional midi parameters of pitch and velocity. There'll be
some dsp required to build the instrument, but playing it becomes just
like playing any other soft synth or sampler plugin. The reason to
buildi instruments is so that you don't /have/ to construct your audio
synthesis chain from scratch each time, sometimes you just want to
load a plugin and start playing.

Like everything in Extempore, though, we're going to build the
instrument in xtlang and compile it at runtime. If you want the simple
'load up a patch and go' experience, then just load the xtlang code
from a file. But if at any stage you want to modify the guts of the
instrument while you're using it, then just bring up the code, change
it around, re-compile it, and you'll hear the results straight away.

This is a also a fairly long and detailed post.  If you're interested
in just /playing/ instruments rather than writing them, you don't need
to know all this and can jump ahead to this post (TODO).  If you want
to come back later to find out in a bit more detail exactly what's
going on with Extempore instruments then this is the place to find out.

* The Hammond organ

The instrument we're going to build in this tutorial is a [[http://en.wikipedia.org/wiki/Hammond_organ][hammond
organ]]. Firstly, because the Hammond organ is an iconic sound---widely
used in many genres of music since its invention in 1934. Any digital
synthesis environment worth it's salt has to provide a hammond patch
of some description :) And secondly, because the hammond organ is
actually not too tricky to sythesize, at least in a simplified way.
The organ's tone is basically the result of the superposition of 9
sinusoids (one for each tonewheel), and so it's a nice way to
introduce the basics of additive synthesis.[fn:quirks]

So why do they call them tonewheel organs, anyway? A
[[http://en.wikipedia.org/wiki/Tonewheel][tonewheel]] is a metal disks (wheel) with a corrugated edge. The disk is
mechanically rotated on it's axis near an electrical pickup, which
'picks up' the changes in the electrical and magnetic fields due to
the rotation of the wheel (and particularly the bumps on the edge of
the wheel). As the bumps go past the pickup, they induce a voltage
which causes a current, which is the audio signal. The frequency
(pitch) of the signal can be changed by altering the rotation speed of
the wheel.

In general, each tonewheel is set up to generate a sine wave. By
having multiple tonewheels of different diameters attached to the same
axle, the organ generates several different sinusoids together, which
allows it to have a more interesting timbre than just a sine tone.

The key tone-shaping control in a Hammond organ are its drawbars,
which look like this:

#+begin_html
<a href=""><img src="images/drawbars.png" alt=""></a>
#+end_html

Each drawbar controls the relative amplitude of a given tonewheel. The
tonewheels are denoted by their 'pipe length', which is a carry-over
from pipe organ design, which Hammond originally developed the
tonewheel organ to be a cheap replacement for. A longer pipe means a
lower pitch, so the drawbars are laid out from low harmonics on the
left to high harmonics on the right. Even though in a tonewheel organ
there aren't any pipes (because they're been replaced by tonewheels!)
the drawbars are still labelled in this way. And anyway, if we're
modelling the organ digitally then there aren't any real tonewheels
either :)

To change the tone of the organ, the organist can adjust the positions
of the drawbars. Fully 'out' (/down/ in this diagram) means that the
frequency associated with that tonewheel is at it's maximum, whereas
fully 'in' (/up/ in this diagram) means that that frequency is silent.
The colours of the drawbar ends also give information about that
harmonic: red drawbars for sub-harmonics, grey for even harmonics and
black for the odd harmonics. Confusingly, by convention the left-most
drawbar isn't the fundamental frequency of the note, it's an octave
below the fundamental (which is controlled by the /third/ drawbar,
indicated by the orange box in the diagram). It's also important to
remember that the drawbars don't represent specific pitches, because
the absolute pitch each drawbar is mapped to depends on the note being
played (in the original tonewheel design, this is controlled by how
fast the axle with the tonewheels on it is rotating). The ratios
between the frequencies are the important part, because they define
how the organ sounds---the organ's /timbre/.

Now, this is probably more information than is absolutely necessary to
construct a simple model of the organ---at a bare minimum all we
really needed to know was that the organ tone is a sum of
sinusoids and the frequency relationships between those sinusoids.
Still, a bit more context is helpful in understanding /why/ the
organ's tone is produced like it is, and helps us think about how to
represent and produce the tone digitally.

* Making a drone organ

The first part of making an instrument is defining its 'drone' tone:
the sound that the instrument makes when it's being sustained. The
kernel is just the sound the instrument would make if it were allowed
to drone on forever without stopping, like if you left a paperweight
on one of the organ's keys.

So, because the basis of the hammond organ tone is the sum of 9
sinusoids (one for each drawbar), then that's what we need to
generate. There are lots of ways to do this, but one nice way is to
use oscillator closures created by Extempore's =make-oscil= function.

#+begin_src extempore
  (load "libs/core/instruments.xtm")
  
  (bind-func organ-drone
    (let ((num_drawbars 9)
          ;; allocate memory for the oscillators and other bits and pieces
          (freq_ratio:double* (zalloc num_drawbars))
          (drawbar_pos:i64* (zalloc num_drawbars))
          (tonewheel:[double,double,double]** (zalloc num_drawbars))
          (i 0))
      ;; fill the allocated memory with the right values
      ;; drawbar frequencies as ratio of fundamental frequency
      (pfill! freq_ratio 0.5 1.5 1.0 2.0 3.0 4.0 5.0 6.0 8.0)
      ;; drawbar positions: 0 = min, 8 = max amplitude
      (pfill! drawbar_pos 8 8 8 0 0 0 0 0 0)
      ;; put an oscilattor into each tonewheel position
      (dotimes (i num_drawbars)
            (pset! tonewheel i (make-oscil 0.0)))
      (lambda (freq)
        (let ((sum 0.0))
          ;; loop over all the drawbars/tonewheels to get the sum
          (dotimes (i num_drawbars)
            (set! sum (+ sum (* (/ (i64tod (pref drawbar_pos i)) 8.0)
                                ((pref tonewheel i) 1.0
                                 (* freq (pref freq_ratio i)))))))
          ;; normalise the sum by the number of drawbars
          (/ sum (i64tod num_drawbars))))))
  
  ;; send the organ drone to the audio sink
  
  (bind-func dsp:DSP
    (lambda (in time chan dat)
      (organ-drone 440.0)))
  
  (dsp:set! dsp)
#+end_src

Compiling the function =organ-drone= does three things:

- *allocate memory* to store the data associated with our sine
  oscillators. For each oscillator, this is =freq_ratio= (the
  frequency relationship to the fundamental), =drawbar_pos= (the
  amplitude of the sine tone) and =tonewheel= (the oscillator closure
  itself).  This data is all stored via [[file:~/Documents/biott/org/_posts/2012-08-13-understanding-pointers-in-xtlang.org][pointers]] to [[file:~/Documents/biott/org/_posts/2012-08-17-memory-management-in-extempore.org][zone memory]]
  through the calls to =zalloc=.
- *fill memory* with the appropriate values. For =freq_ratio= and
  =drawbar_pos=, the values are set 'manually' using =pfill!=, while
  for filling the =tonewheel= buffer =make-oscil= is called in a loop
  (=dotimes=).
- *create & bind a closure* (the =lambda= form) which calculates the
  current output value by calling each of the oscillators in the
  =tonewheel= closure buffer, summing and returning their (normalised)
  return values. This closure is then callable using its name:
  =organ-drone=.

When we call the =organ-drone= closure in the =dsp= callback, we hear
a droning organ tone. It should be really obvious at this point that
the closure =organ-drone= doesn't represent a /pure/ function: one
that stateless and always returns the same output value for a given
input value. If it /were/ a pure function, then calling it in the dsp
callback above with an argument of =200.0= would always return the
same value.  This wouldn't be very interesting in an audio output
scenario---audio is only interesting when the waveforms are
oscillating, and particularly when the oscillations are periodic.
That's basically all pitched sounds are: periodic waveforms. So for
the =organ-drone= closure to produce a nice pitched organ tone, there
must be some state hidden somewhere which is changing and allowing the
closure to return a periodic waveform.

If you guessed that the magic happens in the closures returned by
=make-oscil= (which are in the memory pointed to by =tonewheel=),
you'd be right. Each closure 'closes over' a state variable called
=phase=, which you can see in the source for =make-oscil= (which is in
=libs/core/audio_dsp.xtm=)

#+begin_src extempore
(bind-func make-oscil
  (lambda (phase)
    (lambda (amp freq)
      (let ((inc (* TWOPI (/ freq SAMPLERATE))))
	(set! phase (+ phase inc))
	(if (> phase PI) (set! phase (- phase TWOPI)))
	(* amp (_sin phase))))))
#+end_src

In an xtlang type diagram, =make-oscil= looks like this

#+begin_html
<a href=""><img src="images/make-oscil.png" width="300px" alt=""></a>
#+end_html

=make-oscil= is a higher-order closure, because it returns a closure,
as indicated by the /two/ =lambda= forms: the outer one (with one
=phase= argument) defines the =make-oscil= closure itself, while the
inner one (with =amp= and =freq= arguments) creates the closure which
is returned by =make-oscil=.  /That's/ the closure that gets stored in
the =tonewheel= array when we perform the loop:

#+begin_src extempore
  (dotimes (i num_drawbars)
            (pset! tonewheel i (make-oscil 0.0)))
#+end_src

Looking back up at the =make-oscil= source code, in the body of the
inner lambda there's the line =(set! phase (+ phase inc))= which
increments the value of the =phase= variable based on what the
frequency (=freq=) argument to the closure was. Each closure returned
by =make-oscil= has its own =phase= variable[fn:closure], so calling
one oscillator (and incrementing its phase) doesn't affect the phase
of any other oscillators which might be floating around. This is super
handy, because it allows each oscillator to do its own
'bookkeeping'---keeping track of where it is in its cycle, while
taking more meaningful frequency arguments at 'call-time', so that
they can be easily modulated. This is what allows us to create /buffers/
of closures which we can access and modify via pointers, which is
exactly what we're doing with =tonewheel=.

Going back up to the =organ-drone= above, there's one more point
worth making about closures and scoping. Notice how there's a =let=
outside the =lambda=, which is where the data buffers (=freq_ratio=,
=drawbar_pos= and =tonewheel= are all both allocated (with =zalloc=)
and initialised (with =pfill!= & =pset!=). These data buffers are used
in the body of the =lambda=, so the =lambda= closes over them.

What this means is that these buffers are only allocated and
initialised when the =organ-drone= closure is compiled. When it is
called, on the other hand, the code begins executing from the first
line inside the =lambda= form, which happens to be =(let ((sum 0.0))=.
The values in the =freq_ratio=, =drawbar_pos= and =tonewheel= buffers
will be either in the state they were in when the closure was
compiled, or as they were left by the last closure invocation which
modified them (which, in the case of the =tonewheel= buffer, is
/every/ invocation, because of the call to each oscillator and its
subsequent phase incrementing).

The one argument to the =organ-drone= closure, =freq=, is passed to
every individual oscillator closure in the body of the inner loop,
although it is first modified by the appropriate frequency ratio for
that particular drawbar.  The output value of the closure is then
multiplied by the drawbar position (which is on a scale of 0 to 8,
because the original Hammond organ drawbars had markings from 0 to 8
on each drawbar) to apply the tone-shaping of the drawbars.  After
summing over all the tonewheel oscillators, the (normalised) output
value is then returned.

Because each tonewheel oscillator's frequency is calculated from the
=freq= argument, changing the value of this argument will shift all
the oscillators, just as it should. The harmonic relationships between
the different tonewheel oscillators stays constant, even as the pitch
changes. If you're playing along at home, change the argument from
=440.0= to some other value, recompile it and listen to the difference
in the playback pitch of the organ tone.

# You can even 'reach in' to a given closure to get and set its closed
# over variables using a dot syntax, but its idiomatic extempore to...

* Instruments and note-level control

/Note: you can probably skim over this section if you're not concerned
about the gory details of how Extempore's instrument infrastructure
works. Still, if you've read this far at all then I can probably
assume you have at least some interest :)/

Making this =organ-drone= closure has really just been a prelude to
the real business of making an /instrument/ in Extempore. An Extempore
instrument can be played like a midi soft-synth. Individual notes can
be triggered with an amplitude, a pitch and a duration. Impromptu
users will be pretty familiar with this---it's the same as how you
would play AU synths in Impromptu.[fn:play-note] The only difference
is that the whole signal chain is now written in xtlang and
dynamically compiled at run-time. You can have a look at it in
=libs/core/audio_dsp.xtm= if you want to see the nuts and bolts of how
it works.[fn:dsp-chain]

This notion of /note-level/ control is the key difference between an
Extempore /instrument/ and the type of audio DSP which I've covered in
[[file:~/Documents/biott/org/_posts/2012-06-07-dsp-basics-in-extempore.org][other]] [[file:~/Documents/biott/org/_posts/2012-06-07-more-dsp-and-extempore-types.org][posts]], which were just writing audio continuously to the sound
card through the =dsp= callback. An instrument still needs to be in
the =dsp= callback somewhere: otherwise it can't play its audio out
through the speakers. But it also needs some way of triggering notes
and maintaining the state of all the notes being played at any given
time.  

=define-instrument= takes three arguments: 

1. a name for the instrument 
2. a *note kernel* closure, which must have the [[file:~/Documents/biott/org/_posts/2012-08-09-xtlang-type-reference.org][signature]]
   =[[double,double,double,double,double]*]*=
3. an *effect kernel* closure, which must have the [[file:~/Documents/biott/org/_posts/2012-08-09-xtlang-type-reference.org][signature]]
   =[double,double,double,double,double*]*=

So, when we finally define our hammond organ instrument, the
definition will look like this

#+begin_src extempore
  (define-instrument organ organ-note organ-fx)
#+end_src

and in an [[file:~/Documents/biott/org/_posts/2012-10-03-xtlang-type-diagrams.org][xtlang type diagram]]

#+begin_html
<a href=""><img src="images/full-organ-inst.png" alt=""></a>
#+end_html

=define-instrument= is actually a (Scheme) macro, and it takes the two
kernel closures (=organ-note= and =organ-fx=) and compiles a new
xtlang closure, and binds it to the name =organ=.[fn:notyet] These are
just regular xtlang closures, they just have to have a particular type
signature to allow them to play nicely with the rest of the
=define-instrument= processing chain.

# #+begin_src extempore
#   (println (macro-expand '(define-instrument organ organ-note organ-fx)))
#
#   ;; evaluating the above line (and reformatting the output) gives
#
#   (bind-func organ
#     (let* ((poly:i64 48)
#            (notes:[double,double,double,double]** (zalloc poly))
#            (attack:double 200.0)
#            (decay:double 200.0)
#            (release:double 1000.0)
#            (sustain:double 0.6)
#            (gain 2.0)
#            (ii 0)
#            (note-starts:double* (zalloc poly))
#            (new-note (lambda (start freq dur amp)
#                        (let ((free-note:i64 -1)
#                              (iii 0)
#                              (i 0))
#                          (dotimes (i poly)
#                            (if (> (pref note-starts i) 9999999999998.0)
#                                (set! free-note i)))
#                          (if (> free-note -1)
#                              (let ((note_zone (push_zone (* 1024 10))))
#                                (pset! notes free-note (make-note start freq amp dur attack decay release sustain note-starts free-note (organ-note) note_zone))
#                                (pset! note-starts free-note start)
#                                (pop_zone)
#                                1)
#                              0)))))
#       (dotimes (ii poly)
#         (pset! note-starts ii 9999999999999.0))
#       (lambda (in:double time:double chan:double dat:double*)
#         (let ((out:double 0.0)
#               (k 0))
#           (dotimes (k poly)
#             (if (< (pref note-starts k) time)
#                 (set! out (+ out (* 0.3 ((pref notes k) in time chan))))))
#           (* gain (organ-fx out time chan dat))))))
# #+end_src

So, let's have a look at the lifecycle of a note played on our =organ=
with the help of a few [[file:~/Documents/biott/org/_posts/2012-10-03-xtlang-type-diagrams.org][xtlang type diagrams]]. I'll assume at this point
that =organ= (and therefore =organ-note= and =organ-fx=) have been
successfully compiled, even though they haven't---yet. The xtlang
source code for all the functions I mention are in
=libs/core/instruments.xtm= if you want to see (or redefine) it for
yourself.

The first thing that needs to happen before you can start playing
notes on an Extempore instrument is that the instrument needs to be
called in the =dsp= callback.  If we /only/ want our organ in the
audio output, then that's as simple as

#+begin_src extempore
  (bind-func dsp:DSP
    (lambda (in time chan dat)
      ;; call the organ instrument closure
      (organ in time chan dat)))
  
  (dsp:set! dsp)
#+end_src

Once the DSP closure is set (with =(dsp:set! dsp)=), the =dsp= closure
is called for every audio sample, so in this case the audio output is
just the return value of the =organ= closure. But we /don't/ just want
a constant organ drone this time around, we want to be able to play
notes, and to have silence when notes aren't being played. But how
does the =organ= closure know what its output should be and which
notes it should be playing?

The playing of a note happens through a function called =play-note=.

#+begin_html
<a href=""><img src="images/play-note.png" alt=""></a>
#+end_html

which takes four arguments:

- =time=: the time at which to start playing the note (this can either
  be right =(now)= or at some point in the future)
- =inst=: the instrument to play the note on
- =freq=: the frequency (pitch) of the note
- =amp=: the volume/loudness of the note
- =dur=: the duration of the note

Hopefully you can see how =play-note= provides all the control
required to /schedule/ (via the =time= argument) notes of any pitch,
loudness and duration. All you need to play the =organ= like a MIDI
soft synth.

But how does it work? When =play-note= is called with =organ= as the
instrument, the note kernel =organ-note= is called which returns an
anonymous closure that, when called once per audio sample, will
generate the basic (drone) tone of the instrument. This closure is
then turned into /another/ anonymous closure (which additionally
applies an [[http://en.wikipedia.org/wiki/ADSR_envelope#ADSR_envelope][ADSR envelope]] to the audio output of the note kernel) which
is added to =notes=: a buffer of 'note closures' which is =let=-bound
in the top-level of our =organ= closure. This is how polyphony is
achieved: there's one active note closure in =notes= for each note
which is currently sounding, e.g.if a triad is being played there will
be three active note closures in =notes=.

That's all a bit hard to wrap your head when it's described with
words.  So, here's the same explanation in (pretty) pictures:

#+begin_html
<a href=""><img src="images/note-lifecycle.png" alt=""></a>
#+end_html

Don't be overwhelmed if you don't understand the whole thing---you
don't need to if you just want to play the instrument like a regular
soft synth. In fact, you don't even need to understand it to /write/
an instrument, as long as you follow the template and define your note
kernel and effect kernel with the right type signatures.

TODO: also the diagrams aren't /complete/

# Not sure where this content should go...

# The note kernel is another higher order closure. The reason for this
# is that it returns a closure which exists and will be called to yield
# each sample for the duration of a note (as scheduled by =play-note=).
# This gives us polyphony 'for free', because each note's closure can
# close over the state that it requires to make its drone sound, and the
# closures of different notes won't interfere with each other, they can
# be called one after the other with their outputs summed together. 

# The note kernel doesn't take into account the note envelope, the way
# the amplitude changes over the various stages of a note's lifecycle
# (attack, decay, sustain, and release). That stuff is all handled (and
# can be tweaked) through the top-level instrument's closure, which
# we'll get to later on. The note closure returned by the note kernel
# will basically be the =organ-drone= closure we made in the previous
# section, except with a few minor modifications. The key one is that
# the =organ-drone= closure just took one argument (=freq=), whereas the
# closures returned by =organ-note= have to take /four/ =double=
# arguments and returns a =double=.

* Step two: the note kernel

Back to the task at hand, we need to construct the note and effects
kernels for our hammond organ instrument. Once we have those,
=define-instrument= and =play-note= allow us to play the organ like a
soft synth, which is the goal we've been pursuing since the beginning.

The 'template' for the note kernel and effects kernel is something
like this

#+begin_src extempore
  (bind-func organ-note
    (lambda ()
      (lambda (time:double chan:double freq:double amp:double)
        (cond ((= chan 0.0)
               ;; left channel output goes here
               )
              ((= chan 1.0)
               ;; right channel output goes here
               )
              (else 0.0)))))
  
  (bind-func organ-fx
    (lambda (in:double time:double chan:double dat:double*)
      (cond ((= chan 0.0)
             ;; left channel effects goes here
             )
            ((= chan 1.0)
             ;; right channel effects output goes here
             )
            (else 0.0))))
#+end_src

Notice that we're defining it as a stereo instrument, but that doesn't
mean anything fancier than that we handle the left channel (channel
=0.0=) and the right channel (channel =1.0=) in our =cond= statement.
The generalisation to multi-channel instruments should be
obvious---just use a bigger =cond= form!

To make the =organ-note= kernel, we'll fill in the template from the
=organ-drone= closure we made earlier.

#+begin_src extempore
  (bind-func organ-note
    (let ((num_drawbars 9)
          (freq_ratio:double* (zalloc num_drawbars))
          (drawbar_pos:i64* (zalloc num_drawbars)))
      (pfill! freq_ratio 0.5 1.5 1.0 2.0 3.0 4.0 5.0 6.0 8.0)
      (pfill! drawbar_pos 8 8 8 0 3 0 0 0 0)
      (lambda ()
        (let ((tonewheel:[double,double,double]** (zalloc (* 2 num_drawbars)))
              (freq_smudge:double* (zalloc num_drawbars))
              (i 0))
          (dotimes (i num_drawbars)
            (pset! tonewheel (* i 2) (make-oscil 0.0)) ; left
            (pset! tonewheel (+ (* i 2) 1) (make-oscil 0.0)) ; right
            (pset! freq_smudge i (* 3.0 (random))))
          (lambda (time:double chan:double freq:double amp:double)
            (if (< chan 2.0)
                (let ((sum 0.0))
                  (dotimes (i num_drawbars)
                    (set! sum (+ sum (* (/ (i64tod (pref drawbar_pos i)) 8.0)
                                        ((pref tonewheel (+ (* 2 i) (dtoi64 chan)))
                                         amp
                                         (+ (* freq (pref freq_ratio i))
                                            (pref freq_smudge i)))))))
                  (/ sum (i64tod num_drawbars)))))))))
#+end_src

The general shape of the code is basically the same as in
=organ-drone=. We still allocate a =tonewheel= a buffer of closures to
keep track of our oscillators, and we still sum them all together with
relative amplitudes based on the drawbar position. There are just
additions:

- the instrument is now stereo, so the =tonewheel= buffer is now twice as
  big (=(zalloc (* 2 num_drawbars))=).  This gives us two oscillator
  closures per tonewheel, one for L and one for R.
- a 'smudge factor' (=freq_smudge=) has been added to the tonewheel
  frequencies.  This is to make it sound a bit more 'organic', because
  in a physical instrumet the frequency ratios between the tonewheels
  aren't perfect.

The other important difference between =organ-note= and =organ-drone=
is that while =organ-drone= returns a double value (and so can be
called directly for playback in the =dsp= closure), =organ-note=
returns a /closure/.  A type diagram highlights the difference:

#+begin_html
<a href=""><img src="images/organ-drone-vs-note.png" alt=""></a>
#+end_html

As I described in the previous section, this provides the flexibility
required to manage note scheduling (via =play-note=) and polyphony.

* Step three: the effect kernel

The final piece of the puzzle is the effect kernel =organ-fx=.
In a Hammond organ, the frequencies of the
different tonewheels are related to the


[fn:quirks] Any commercial Hammond organ modelling synth will add
/heaps/ of other stuff to this basic tone, to faithfully recreate the
nuances and quirks of the real physical instrument, even down to the
details of the specific model being emulated. We won't try to do too
much of that in this tutorial, but again, if you want to hack around
add things to the instrument then feel free.

[fn:play-note] In fact, there's a =play-note= function and a =play=
macro in Extempore which are exactly the same as they were in
Impromptu---so any audio code which triggered notes in this way should
work unmodified (although the synth which receives and /plays/ these
notes will be different).

[fn:dsp-chain] [[file:~/Documents/biott/org/_posts/2012-06-07-dsp-basics-in-extempore.org][This post]] covers the DSP architecture in Extempore if
you're looking for more background.

[fn:closure] This is why they're called closures, because they /close
over/ their non-local variables.

[fn:notyet] We can't actually evaluate this instrument definition yet, because we
haven't yet defined the note and effect kernels.
